Excellent â€” now that you have the official specification, we can tighten and upgrade your plan so it perfectly aligns with what judges expect and covers all evaluation dimensions.

Below is a refined roadmap, showing what to add, adjust, or simplify from your current version.

â¸»

ğŸš€ Refined Strategy for â€œAutomated Meta-Analysis & Systematic Reviewâ€

â¸»

1ï¸âƒ£ Overall System Vision

Keep the same modular agentic architecture â€” but now explicitly align each agent to the official pipeline terms:

Official Stage	Your Module	Key Changes / Additions
SEARCH	SearchAgent	Must handle at least 3 sources: PubMed (via E-utilities), bioRxiv/medRxiv (via API or SerpAPI), and Google Scholar fallback. Log full query string + timestamp (for reproducibility).
SELECT	ScreeningAgent	Implement inclusion/exclusion criteria parsing + relevance scoring (e.g., 0â€“1 relevance). Add quality scoring (Cochrane â€œrisk of biasâ€ or PRISMA traffic-light rating).
EXTRACT	ExtractionAgent	Must extract numeric data (n, mean, SD, effect size, CI). Add classification of: study type (RCT/cohort/case-control), species, and data type (biochemistry, RNAseq, etc.).
ANALYZE	StatsAgent	Implement random/fixed effects meta-analysis (e.g. metafor, statsmodels, or rpy2). Include heterogeneity (IÂ², Ï„Â²) and funnel plot generation.
REPORT (implicit)	ReportAgent	Generate PRISMA-style flow diagram + structured summary (intervention, sample, methods, effect, quality).


â¸»

2ï¸âƒ£ Additions to Meet â€œTECHNICAL_REQUIREMENTSâ€

âœ… Automated Classification

For every included study, classify:
	â€¢	Article Type: original / review / meta-analysis / case report (LLM or text-classifier)
	â€¢	Study Design: RCT / cohort / case-control
	â€¢	Species: Homo sapiens / Mus musculus
	â€¢	Data Type: molecular / behavioral / imaging / etc.

Use a lightweight ClassificationAgent or extend ExtractionAgent with a taxonomy JSON schema.

âœ… Study Design Handling
	â€¢	Differentiate statistical models per design:
	â€¢	RCT â†’ standardized mean difference (SMD)
	â€¢	Cohort â†’ relative risk (RR) or hazard ratio (HR)
	â€¢	Case-control â†’ odds ratio (OR)

This can be a function switch in the StatsAgent.

âœ… Quality Assessment

Implement a Confidence Score:

{
  "bias_risk": "low",
  "journal_rank": "Q1",
  "confidence_score": 0.85
}

Use journal metadata (e.g. via CrossRef API) to enrich.

â¸»

3ï¸âƒ£ Evaluation Framework Alignment

You must design your pipeline so each 25% category can be quantitatively measured.

Evaluation Criterion	How to Measure in Prototype
Study Selection Accuracy	Compare your selected DOIs vs. provided gold-standard list â†’ precision/recall.
Data Extraction Accuracy	Compare your extracted values vs. provided dataset (RMSE, mean diff).
Statistical Validity	Compare meta-analysis result (effect size, CI) vs. published one.
Time Efficiency	Log timestamps â†’ compute runtime vs. estimated human effort (from Cochrane average).

Include a small â€œevaluation notebookâ€ to compute these metrics.

â¸»

4ï¸âƒ£ Advanced Capabilities (for bonus points)

Bonus Feature	Easy Implementation Hint
âš¡ Real-time updating	Re-run PubMed query weekly; highlight new articles vs last version (hash comparison).
âš¡ Interactive dashboard	Use Streamlit or Gradio to visualize forest/funnel plots and subgroup filters.
âš¡ Regulatory-ready output	Export report in structured XML (PRISMA-compliant schema).


â¸»

5ï¸âƒ£ Adapted Project Folder Structure

meta-ai/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ search_agent.py
â”‚   â”œâ”€â”€ screening_agent.py
â”‚   â”œâ”€â”€ extraction_agent.py
â”‚   â”œâ”€â”€ classification_agent.py
â”‚   â”œâ”€â”€ stats_agent.py
â”‚   â””â”€â”€ report_agent.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ test_case_1_metformin/
â”‚   â”œâ”€â”€ test_case_2_behavioral/
â”‚   â””â”€â”€ test_case_3_diagnostic/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ evaluation_metrics.ipynb
â”‚   â”œâ”€â”€ forest_plot.ipynb
â”‚   â””â”€â”€ heterogeneity_analysis.ipynb
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ONE-PAGER.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ PRISMA_FLOW.png
â”‚   â””â”€â”€ QUESTIONS.md
â”œâ”€â”€ meta_pipeline.py          # orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»

6ï¸âƒ£ Refined Step Flow
	1.	User Input: "What is the effect of metformin on lifespan in animal models?"
	2.	SearchAgent â†’ fetch top 100 results (PubMed + bioRxiv)
	3.	ScreeningAgent â†’ apply PRISMA inclusion/exclusion â†’ produce shortlist
	4.	ExtractionAgent â†’ parse tables + extract effect sizes (mean, SD, n)
	5.	ClassificationAgent â†’ annotate article type, species, study design
	6.	StatsAgent â†’ compute effect size, heterogeneity, funnel + forest plots
	7.	ReportAgent â†’ generate structured report (PRISMA flow, summary table)
	8.	Evaluation Notebook â†’ compare with ground truth for scoring

â¸»

7ï¸âƒ£ Team Adjustments

Role	New Focus
PM / Domain Lead	Define inclusion/exclusion & quality rules (PRISMA-based)
ML Engineer	Implement classification and LLM-based extraction
Data Scientist	Focus on heterogeneity stats and validation
DevOps	Create reproducible pipeline + logging
Communicator	Prepare demo, README, and visualization dashboard


â¸»

8ï¸âƒ£ Key Improvements from Your Original Plan

Old Plan	Adjustment
4 main agents	Expand to 5â€“6 (add Classification & Quality)
MVP = forest plot	MVP = PRISMA-style structured report
Manual question choice	Must include 3 provided test cases
Simple evaluation	Add quantitative metrics per official framework
General pipeline	Add study design-specific handling
Unstructured output	Produce JSON + PDF + Dashboard output


â¸»


# Requirements

SEARCH	SearchAgent	Must handle at least 3 sources: PubMed (via E-utilities), bioRxiv/medRxiv (via API or SerpAPI), and Google Scholar fallback. Log full query string + timestamp (for reproducibility).

1. User Input: "What is the effect of metformin on lifespan in animal models?" 
- assuming we have a valid inquiery
2. SearchAgent â†’ fetch top 100 results (PubMed + bioRxiv)
- api requests to 3 sources: PubMed (via E-utilities), bioRxiv/medRxiv (via API or SerpAPI), and Google Scholar fallback. Log full query string + timestamp (for reproducibility).









AI inquiery formatter. 
- a user send research inquiry per prompt
- if the inquiery is not clear - AI (e.g. Chat GPT) offers formatting